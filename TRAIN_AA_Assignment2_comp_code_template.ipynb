{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By9aWNscEHgx"
      },
      "source": [
        "### Test the Model (Competition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWnLtBuWGN6k",
        "outputId": "8a576fc2-c049-4955-8dbe-4d74ca58440b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.54.1)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.66.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.4.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch\n",
        "%pip install transformers\n",
        "%pip install scikit-learn\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install tqdm\n",
        "%pip install joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ti_TW4zEEHgy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import get_scheduler\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PMO3kwq8EHgz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('AA_movie_train_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0llv7sfEHgz",
        "outputId": "abc35762-f15e-4e32-d7d3-83d81781f083"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['action', 'comedy', 'documentary', 'drama', 'thriller']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#movie_genre = list(df['Genre'].unique())\n",
        "#movie_genre.sort()\n",
        "#movie_genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "iT4AJ0DpEHg0",
        "outputId": "a8fc1fe0-64b4-42ba-a718-0b94d636ba83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Title          False\n",
              "Genre          False\n",
              "Description    False\n",
              "dtype: bool"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQEcjZqXEHg0",
        "outputId": "7e6c4d63-7047-428f-b232-75ab52d91425"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5000, 3)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRclHdigEHg0",
        "outputId": "39409591-063d-4363-8a13-f74cfcbcb85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Required Libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. fit on Genre strings\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['Genre'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1whO1i4YX3G"
      },
      "outputs": [],
      "source": [
        "# 2. train-test split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['Description'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "# 3. tokenization using BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n",
        "\n",
        "# 4. create PyTorch Datasets\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = MovieDataset(train_encodings, train_labels)\n",
        "val_dataset = MovieDataset(val_encodings, val_labels)\n",
        "\n",
        "\n",
        "# 6️. Optimizer & Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 3\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "\n",
        "# 7. training\n",
        "for epoch in range(num_epochs):\n",
        "  # 5. Model initialization\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        total_loss += loss.item() * preds.size(0)\n",
        "        total_correct += (preds == batch['labels']).sum().item()\n",
        "        total_samples += preds.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    train_accuracy = total_correct / total_samples\n",
        "    train_loss = total_loss / total_samples\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            val_loss += loss.item() * preds.size(0)\n",
        "            val_correct += (preds == batch['labels']).sum().item()\n",
        "            val_samples += preds.size(0)\n",
        "\n",
        "    val_accuracy = val_correct / val_samples\n",
        "    val_loss = val_loss / val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:  \"\n",
        "          f\"accuracy: {train_accuracy:.4f} - loss: {train_loss:.4f} - \"\n",
        "          f\"val_accuracy: {val_accuracy:.4f} - val_loss: {val_loss:.4f}\")\n",
        "\n",
        "# 9. evaluation\n",
        "model.eval()\n",
        "train_preds, train_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        train_preds.extend(preds.cpu().numpy())\n",
        "        train_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_preds, val_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        val_preds.extend(preds.cpu().numpy())\n",
        "        val_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "val_acc = accuracy_score(val_labels, val_preds)\n",
        "f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "\n",
        "# Classification report\n",
        "target_names = label_encoder.inverse_transform(np.unique(val_labels))\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc}\")\n",
        "print(f\"Validation Accuracy: {val_acc:}\")\n",
        "print(f\"F1 Score (Macro): {f1}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(val_labels, val_preds, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-v6-LVX8bu1",
        "outputId": "ac14cea9-0a75-4992-b887-fe566f2cef23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./bert_genre_classifier/tokenizer_config.json',\n",
              " './bert_genre_classifier/special_tokens_map.json',\n",
              " './bert_genre_classifier/vocab.txt',\n",
              " './bert_genre_classifier/added_tokens.json')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 10. save model in huggingface format\n",
        "model.save_pretrained(\"./bert_genre_classifier\")\n",
        "tokenizer.save_pretrained(\"./bert_genre_classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY2SWSZjJCTW",
        "outputId": "81fe7072-119b-451a-a6b3-00cb25a2aeb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['./bert_genre_classifier/label_encoder.pkl']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "joblib.dump(label_encoder, \"./bert_genre_classifier/label_encoder.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMVwjSKO6zfE",
        "outputId": "6d533eef-5197-4a41-ca6b-f1fbc5f74868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Accuracy: 1.0\n",
            "F1 Score (macro): 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      action       1.00      1.00      1.00         1\n",
            "      comedy       1.00      1.00      1.00         1\n",
            " documentary       1.00      1.00      1.00         1\n",
            "       drama       1.00      1.00      1.00         1\n",
            "    thriller       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         5\n",
            "   macro avg       1.00      1.00      1.00         5\n",
            "weighted avg       1.00      1.00      1.00         5\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#### code template test\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import joblib\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"./bert_genre_classifier\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"./bert_genre_classifier\")\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# load unseen dataset\n",
        "COMP_DATASET = \"AA_movie_comp_template.csv\" # replace with path\n",
        "new_df = pd.read_csv(COMP_DATASET)\n",
        "\n",
        "\n",
        "# fit on Genre strings\n",
        "movie_genre = ['action', 'comedy', 'documentary', 'drama', 'thriller'] #taken from training output\n",
        "label_encoder = joblib.load(\"./bert_genre_classifier/label_encoder.pkl\")\n",
        "#label_encoder.classes_ = np.array(movie_genre)\n",
        "new_df['label'] = label_encoder.transform(new_df['Genre'])\n",
        "\n",
        "texts = new_df[\"Description\"].tolist()\n",
        "\n",
        "# tokenize\n",
        "encodings = tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
        "encodings = {k: v.to(device) for k, v in encodings.items()}\n",
        "\n",
        "# predict\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encodings)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "# Move predictions to CPU and convert to numpy before inverse_transform\n",
        "predicted_genres = label_encoder.inverse_transform(predictions.cpu().numpy())\n",
        "\n",
        "# save predictions to new CSV\n",
        "new_df[\"predicted_genre\"] = predicted_genres\n",
        "new_df.to_csv(\"predicted_genres.csv\", index=False)\n",
        "\n",
        "# evaluate\n",
        "true_labels = np.array(new_df['label'].tolist()) # Convert true_labels to a NumPy array\n",
        "accuracy = accuracy_score(true_labels, predictions.cpu().numpy())\n",
        "f1 = f1_score(true_labels, predictions.cpu().numpy(), average='macro')\n",
        "report = classification_report(true_labels, predictions.cpu().numpy(), target_names=label_encoder.classes_)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:}\")\n",
        "print(f\"F1 Score (macro): {f1:}\")\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LvesjsY4qT3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fbNa0V3coFl"
      },
      "source": [
        "## Hyperparam Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xR2FeE-XzAB"
      },
      "source": [
        "### 1. Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX6bbMssYCTp",
        "outputId": "045fa21b-855b-49f0-9f30-b4bd643982ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/10:   0%|          | 0/250 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "Epoch 1/10: 100%|██████████| 250/250 [02:45<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10:  accuracy: 0.6332 - loss: 0.9477 - val_accuracy: 0.7770 - val_loss: 0.6323\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 250/250 [02:45<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10:  accuracy: 0.8367 - loss: 0.4824 - val_accuracy: 0.7510 - val_loss: 0.7088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 250/250 [02:45<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10:  accuracy: 0.9307 - loss: 0.2223 - val_accuracy: 0.7750 - val_loss: 0.7433\n",
            "Early stopping triggered at epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.984\n",
            "Validation Accuracy: 0.775\n",
            "F1 Score (Macro): 0.7760152891260075\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      action       0.82      0.75      0.78       200\n",
            "      comedy       0.80      0.69      0.74       200\n",
            " documentary       0.93      0.87      0.90       200\n",
            "       drama       0.66      0.72      0.69       200\n",
            "    thriller       0.71      0.85      0.77       200\n",
            "\n",
            "    accuracy                           0.78      1000\n",
            "   macro avg       0.78      0.78      0.78      1000\n",
            "weighted avg       0.78      0.78      0.78      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. train-test split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['Description'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "# 3. tokenization using BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n",
        "\n",
        "# 4. create PyTorch Datasets\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = MovieDataset(train_encodings, train_labels)\n",
        "val_dataset = MovieDataset(val_encodings, val_labels)\n",
        "\n",
        "# 5. Model initialization\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "model.to(device)\n",
        "\n",
        "# 6️. Optimizer & Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 10\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience = 2\n",
        "counter = 0\n",
        "\n",
        "# 7. training\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        total_loss += loss.item() * preds.size(0)\n",
        "        total_correct += (preds == batch['labels']).sum().item()\n",
        "        total_samples += preds.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    train_accuracy = total_correct / total_samples\n",
        "    train_loss = total_loss / total_samples\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            val_loss += loss.item() * preds.size(0)\n",
        "            val_correct += (preds == batch['labels']).sum().item()\n",
        "            val_samples += preds.size(0)\n",
        "\n",
        "    val_accuracy = val_correct / val_samples\n",
        "    val_loss = val_loss / val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:  \"\n",
        "          f\"accuracy: {train_accuracy:.4f} - loss: {train_loss:.4f} - \"\n",
        "          f\"val_accuracy: {val_accuracy:.4f} - val_loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")  # Optional: save best model\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# 9. evaluation\n",
        "model.eval()\n",
        "train_preds, train_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        train_preds.extend(preds.cpu().numpy())\n",
        "        train_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_preds, val_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        val_preds.extend(preds.cpu().numpy())\n",
        "        val_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "val_acc = accuracy_score(val_labels, val_preds)\n",
        "f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "\n",
        "# Classification report\n",
        "target_names = label_encoder.inverse_transform(np.unique(val_labels))\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc}\")\n",
        "print(f\"Validation Accuracy: {val_acc:}\")\n",
        "print(f\"F1 Score (Macro): {f1}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(val_labels, val_preds, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0dDo-x734m-"
      },
      "source": [
        "### 2. comparison with flat learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNJoNIHc5Jma",
        "outputId": "d25e9284-197d-4d56-c6fb-b401bc1ecfb6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/3:   0%|          | 0/250 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "Epoch 1/3: 100%|██████████| 250/250 [02:45<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3:  accuracy: 0.6635 - loss: 0.9120 - val_accuracy: 0.7250 - val_loss: 0.7479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 250/250 [02:45<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/3:  accuracy: 0.8385 - loss: 0.4752 - val_accuracy: 0.7560 - val_loss: 0.7341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|██████████| 250/250 [02:45<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3:  accuracy: 0.9230 - loss: 0.2344 - val_accuracy: 0.7390 - val_loss: 0.8958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== FLAT LEARNING RATE ==\n",
            "Train Accuracy: 0.97075\n",
            "Validation Accuracy: 0.739\n",
            "F1 Score (Macro): 0.7384879352049007\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      action       0.81      0.70      0.75       200\n",
            "      comedy       0.87      0.58      0.69       200\n",
            " documentary       0.86      0.90      0.88       200\n",
            "       drama       0.62      0.68      0.65       200\n",
            "    thriller       0.63      0.84      0.72       200\n",
            "\n",
            "    accuracy                           0.74      1000\n",
            "   macro avg       0.76      0.74      0.74      1000\n",
            "weighted avg       0.76      0.74      0.74      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. train-test split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['Description'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "# 3. tokenization using BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n",
        "\n",
        "# 4. create PyTorch Datasets\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = MovieDataset(train_encodings, train_labels)\n",
        "val_dataset = MovieDataset(val_encodings, val_labels)\n",
        "\n",
        "# 5. Model initialization\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "model.to(device)\n",
        "\n",
        "# 6️. Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 3\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# 7. training\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        total_loss += loss.item() * preds.size(0)\n",
        "        total_correct += (preds == batch['labels']).sum().item()\n",
        "        total_samples += preds.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    train_accuracy = total_correct / total_samples\n",
        "    train_loss = total_loss / total_samples\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            val_loss += loss.item() * preds.size(0)\n",
        "            val_correct += (preds == batch['labels']).sum().item()\n",
        "            val_samples += preds.size(0)\n",
        "\n",
        "    val_accuracy = val_correct / val_samples\n",
        "    val_loss = val_loss / val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:  \"\n",
        "          f\"accuracy: {train_accuracy:.4f} - loss: {train_loss:.4f} - \"\n",
        "          f\"val_accuracy: {val_accuracy:.4f} - val_loss: {val_loss:.4f}\")\n",
        "\n",
        "# 9. evaluation\n",
        "model.eval()\n",
        "train_preds, train_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        train_preds.extend(preds.cpu().numpy())\n",
        "        train_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_preds, val_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        val_preds.extend(preds.cpu().numpy())\n",
        "        val_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "val_acc = accuracy_score(val_labels, val_preds)\n",
        "f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "\n",
        "# Classification report\n",
        "target_names = label_encoder.inverse_transform(np.unique(val_labels))\n",
        "\n",
        "print(f\"== FLAT LEARNING RATE ==\")\n",
        "print(f\"Train Accuracy: {train_acc}\")\n",
        "print(f\"Validation Accuracy: {val_acc:}\")\n",
        "print(f\"F1 Score (Macro): {f1}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(val_labels, val_preds, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEPvvSgb4u93"
      },
      "source": [
        "### 3. automated tuning w optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zyb0AzK6xzV",
        "outputId": "bedaba40-0ab8-4119-e866-c6ee23e954db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.4.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fGgycCEF3-E2",
        "outputId": "df05e36f-205b-4134-c8ce-d33bcc965af2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:07:02,786] A new study created in memory with name: no-name-342fbe9e-f712-4822-b502-d70bab3c518c\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 03:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.763200</td>\n",
              "      <td>0.679775</td>\n",
              "      <td>0.750034</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.395900</td>\n",
              "      <td>0.691220</td>\n",
              "      <td>0.755082</td>\n",
              "      <td>0.757000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.161800</td>\n",
              "      <td>0.777364</td>\n",
              "      <td>0.773098</td>\n",
              "      <td>0.773000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:11:00,805] Trial 0 finished with value: 0.773097765908797 and parameters: {'learning_rate': 4.288736962320381e-05, 'batch_size': 16, 'num_train_epochs': 3}. Best is trial 0 with value: 0.773097765908797.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 03:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.789900</td>\n",
              "      <td>0.728059</td>\n",
              "      <td>0.752236</td>\n",
              "      <td>0.753000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.381100</td>\n",
              "      <td>0.700401</td>\n",
              "      <td>0.763067</td>\n",
              "      <td>0.764000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.134100</td>\n",
              "      <td>0.880800</td>\n",
              "      <td>0.788629</td>\n",
              "      <td>0.789000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:14:54,178] Trial 1 finished with value: 0.7886292113867858 and parameters: {'learning_rate': 7.722228565978238e-05, 'batch_size': 16, 'num_train_epochs': 3}. Best is trial 1 with value: 0.7886292113867858.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 06:03, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.057400</td>\n",
              "      <td>0.683781</td>\n",
              "      <td>0.750792</td>\n",
              "      <td>0.757000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.595600</td>\n",
              "      <td>0.644385</td>\n",
              "      <td>0.768441</td>\n",
              "      <td>0.770000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.413100</td>\n",
              "      <td>0.689104</td>\n",
              "      <td>0.766176</td>\n",
              "      <td>0.767000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.150200</td>\n",
              "      <td>0.753215</td>\n",
              "      <td>0.768668</td>\n",
              "      <td>0.769000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.085200</td>\n",
              "      <td>0.783308</td>\n",
              "      <td>0.778467</td>\n",
              "      <td>0.778000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:21:05,071] Trial 2 finished with value: 0.7784671425619185 and parameters: {'learning_rate': 3.19275020871754e-05, 'batch_size': 32, 'num_train_epochs': 5}. Best is trial 1 with value: 0.7886292113867858.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 03:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.970100</td>\n",
              "      <td>0.707754</td>\n",
              "      <td>0.735036</td>\n",
              "      <td>0.741000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.507400</td>\n",
              "      <td>0.663459</td>\n",
              "      <td>0.765199</td>\n",
              "      <td>0.766000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.307500</td>\n",
              "      <td>0.712425</td>\n",
              "      <td>0.774710</td>\n",
              "      <td>0.775000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:24:48,347] Trial 3 finished with value: 0.7747100710126997 and parameters: {'learning_rate': 6.825959893441374e-05, 'batch_size': 32, 'num_train_epochs': 3}. Best is trial 1 with value: 0.7886292113867858.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 05:07, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.777100</td>\n",
              "      <td>0.643553</td>\n",
              "      <td>0.774614</td>\n",
              "      <td>0.775000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.407900</td>\n",
              "      <td>0.652774</td>\n",
              "      <td>0.781149</td>\n",
              "      <td>0.782000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.160800</td>\n",
              "      <td>0.833281</td>\n",
              "      <td>0.779849</td>\n",
              "      <td>0.781000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.068200</td>\n",
              "      <td>0.940284</td>\n",
              "      <td>0.785156</td>\n",
              "      <td>0.784000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:30:03,157] Trial 4 finished with value: 0.7851556308389875 and parameters: {'learning_rate': 4.13406615869407e-05, 'batch_size': 16, 'num_train_epochs': 4}. Best is trial 1 with value: 0.7886292113867858.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 05:10, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.795400</td>\n",
              "      <td>0.700546</td>\n",
              "      <td>0.756374</td>\n",
              "      <td>0.756000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.396700</td>\n",
              "      <td>0.749326</td>\n",
              "      <td>0.759697</td>\n",
              "      <td>0.761000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.188100</td>\n",
              "      <td>0.966585</td>\n",
              "      <td>0.760647</td>\n",
              "      <td>0.764000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.061600</td>\n",
              "      <td>1.151745</td>\n",
              "      <td>0.765286</td>\n",
              "      <td>0.765000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:35:21,091] Trial 5 finished with value: 0.76528581800863 and parameters: {'learning_rate': 6.613447523059401e-05, 'batch_size': 16, 'num_train_epochs': 4}. Best is trial 1 with value: 0.7886292113867858.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 03:55, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.793000</td>\n",
              "      <td>0.769485</td>\n",
              "      <td>0.721358</td>\n",
              "      <td>0.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.376100</td>\n",
              "      <td>0.728540</td>\n",
              "      <td>0.761672</td>\n",
              "      <td>0.762000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.130100</td>\n",
              "      <td>0.939323</td>\n",
              "      <td>0.768336</td>\n",
              "      <td>0.768000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:39:24,705] Trial 6 finished with value: 0.768335692012269 and parameters: {'learning_rate': 7.897679066069245e-05, 'batch_size': 16, 'num_train_epochs': 3}. Best is trial 1 with value: 0.7886292113867858.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 06:08, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.771800</td>\n",
              "      <td>0.667808</td>\n",
              "      <td>0.757781</td>\n",
              "      <td>0.759000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.413100</td>\n",
              "      <td>0.660080</td>\n",
              "      <td>0.782459</td>\n",
              "      <td>0.782000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.172700</td>\n",
              "      <td>0.884864</td>\n",
              "      <td>0.760051</td>\n",
              "      <td>0.761000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.069800</td>\n",
              "      <td>1.107991</td>\n",
              "      <td>0.759018</td>\n",
              "      <td>0.759000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.028100</td>\n",
              "      <td>1.130764</td>\n",
              "      <td>0.763881</td>\n",
              "      <td>0.763000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:45:40,408] Trial 7 finished with value: 0.7638810412232551 and parameters: {'learning_rate': 4.1344959469258915e-05, 'batch_size': 16, 'num_train_epochs': 5}. Best is trial 1 with value: 0.7886292113867858.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 03:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.005900</td>\n",
              "      <td>0.695996</td>\n",
              "      <td>0.739228</td>\n",
              "      <td>0.746000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.541400</td>\n",
              "      <td>0.631043</td>\n",
              "      <td>0.768402</td>\n",
              "      <td>0.769000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.364500</td>\n",
              "      <td>0.655901</td>\n",
              "      <td>0.780628</td>\n",
              "      <td>0.781000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:49:11,228] Trial 8 finished with value: 0.7806278654793348 and parameters: {'learning_rate': 4.405721962094015e-05, 'batch_size': 32, 'num_train_epochs': 3}. Best is trial 1 with value: 0.7886292113867858.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 05:33, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.960500</td>\n",
              "      <td>0.688836</td>\n",
              "      <td>0.749904</td>\n",
              "      <td>0.755000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.517900</td>\n",
              "      <td>0.647131</td>\n",
              "      <td>0.770906</td>\n",
              "      <td>0.771000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.311500</td>\n",
              "      <td>0.755673</td>\n",
              "      <td>0.783683</td>\n",
              "      <td>0.784000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.938316</td>\n",
              "      <td>0.772352</td>\n",
              "      <td>0.773000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.024000</td>\n",
              "      <td>1.012746</td>\n",
              "      <td>0.772296</td>\n",
              "      <td>0.771000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-05 14:54:52,262] Trial 9 finished with value: 0.7722959280626938 and parameters: {'learning_rate': 6.4198623939046e-05, 'batch_size': 32, 'num_train_epochs': 5}. Best is trial 1 with value: 0.7886292113867858.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'learning_rate': 7.722228565978238e-05, 'batch_size': 16, 'num_train_epochs': 3}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['Description'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "# 3. tokenization using BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n",
        "\n",
        "# 4. create PyTorch Datasets\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = MovieDataset(train_encodings, train_labels)\n",
        "val_dataset = MovieDataset(val_encodings, val_labels)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # Move tensors to CPU before converting to numpy\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    labels_cpu = labels\n",
        "    f1 = f1_score(labels_cpu, predictions, average=\"macro\")\n",
        "    accuracy = accuracy_score(labels_cpu, predictions)\n",
        "    return {\"eval_f1\": f1, \"eval_accuracy\": accuracy}\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "    model.to(device)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 3e-5, 8e-5, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
        "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 5)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        output_dir=\"./results\",\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        metric_for_best_model=\"eval_f1\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\", # Disable reporting to wandb\n",
        "        save_total_limit=1,\n",
        "        fp16=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train() # Train the model\n",
        "\n",
        "    result = trainer.evaluate()\n",
        "    return result[\"eval_f1\"]\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\",\n",
        "                            pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=0))\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(study.best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCjb_HhWysQY"
      },
      "source": [
        "best params from optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "mKAYIxvdjkOs",
        "outputId": "3e0634ce-1d1b-4570-e2ba-36a9f3b93ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Learning Rate: 7.722228565978238e-05\n",
            "Best Batch Size: 16\n",
            "Best Number of Training Epochs: 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 04:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.706920</td>\n",
              "      <td>0.753133</td>\n",
              "      <td>0.754000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.639900</td>\n",
              "      <td>0.725174</td>\n",
              "      <td>0.755841</td>\n",
              "      <td>0.759000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.639900</td>\n",
              "      <td>0.946137</td>\n",
              "      <td>0.771284</td>\n",
              "      <td>0.770000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 learning rate: 2.05926095092753e-07\n",
            "Epoch 2 learning rate: 2.05926095092753e-07\n",
            "Epoch 3 learning rate: 2.05926095092753e-07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_f1': 0.771284361423979, 'eval_accuracy': 0.77, 'eval_loss': 0.9461365938186646, 'eval_runtime': 3.4648, 'eval_samples_per_second': 288.62, 'eval_steps_per_second': 18.183, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Get the best hyperparameters from the Optuna study\n",
        "best_learning_rate = study.best_params['learning_rate']\n",
        "best_batch_size = study.best_params['batch_size']\n",
        "best_num_train_epochs = study.best_params['num_train_epochs']\n",
        "\n",
        "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
        "print(f\"Best Batch Size: {best_batch_size}\")\n",
        "print(f\"Best Number of Training Epochs: {best_num_train_epochs}\")\n",
        "\n",
        "# 5. Model initialization (re-initialize for training with best params)\n",
        "# Make sure to load the model again if you want to train from scratch\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "\n",
        "# Define compute_metrics function for Trainer\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    f1 = f1_score(labels, predictions, average=\"macro\")\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"eval_f1\": f1, \"eval_accuracy\": accuracy}\n",
        "\n",
        "# 6. Training Arguments with best hyperparameters and fp16 enabled\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_best_params\",\n",
        "    learning_rate=best_learning_rate,\n",
        "    per_device_train_batch_size=best_batch_size,\n",
        "    per_device_eval_batch_size=best_batch_size, # Use best batch size for evaluation as well\n",
        "    num_train_epochs=best_num_train_epochs,\n",
        "    weight_decay=0.01, # Added a small weight decay for regularization\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    # Removed load_best_model_at_last_k_epochs\n",
        "    metric_for_best_model=\"eval_f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=torch.cuda.is_available(), # Enable fp16 if CUDA is available\n",
        "    report_to=\"none\" # Disable reporting to wandb\n",
        ")\n",
        "\n",
        "# 7. Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 8. Train the model\n",
        "# Modify the train function to print the learning rate at the end of each epoch\n",
        "original_train = trainer.train\n",
        "def train_with_lr_print(*args, **kwargs):\n",
        "    original_train(*args, **kwargs)\n",
        "    for epoch in range(int(trainer.state.epoch)):\n",
        "        print(f\"Epoch {epoch+1} learning rate: {trainer.lr_scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "trainer.train = train_with_lr_print\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# 9. Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "# Optional: Save the best model\n",
        "trainer.save_model(\"./best_model_best_params\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "zG0K7O_pwC-t",
        "outputId": "bbe0b195-272f-4007-ef92-cda1f8566678"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Accuracy: 0.985\n",
            "\n",
            "Validation Accuracy: 0.77\n",
            "Validation F1 Score (Macro): 0.771284361423979\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      action       0.78      0.77      0.77       200\n",
            "      comedy       0.83      0.67      0.74       200\n",
            " documentary       0.91      0.90      0.90       200\n",
            "       drama       0.62      0.71      0.66       200\n",
            "    thriller       0.75      0.80      0.78       200\n",
            "\n",
            "    accuracy                           0.77      1000\n",
            "   macro avg       0.78      0.77      0.77      1000\n",
            "weighted avg       0.78      0.77      0.77      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get predictions and true labels for classification report\n",
        "train_predictions = trainer.predict(train_dataset).predictions\n",
        "train_predicted_labels = np.argmax(train_predictions, axis=-1)\n",
        "train_true_labels = train_dataset.labels\n",
        "\n",
        "val_predictions = trainer.predict(val_dataset).predictions\n",
        "val_predicted_labels = np.argmax(val_predictions, axis=-1)\n",
        "val_true_labels = val_dataset.labels\n",
        "\n",
        "\n",
        "# Calculate and print train accuracy and F1 score\n",
        "train_accuracy = accuracy_score(train_true_labels, train_predicted_labels)\n",
        "train_f1 = f1_score(train_true_labels, train_predicted_labels, average='macro')\n",
        "\n",
        "print(f\"\\nTrain Accuracy: {train_accuracy}\")\n",
        "\n",
        "# Calculate and print validation accuracy and F1 score\n",
        "val_accuracy = accuracy_score(val_true_labels, val_predicted_labels)\n",
        "val_f1 = f1_score(val_true_labels, val_predicted_labels, average='macro')\n",
        "\n",
        "print(f\"\\nValidation Accuracy: {val_accuracy}\")\n",
        "print(f\"Validation F1 Score (Macro): {val_f1}\")\n",
        "\n",
        "# Generate and print classification report\n",
        "target_names = label_encoder.classes_\n",
        "report = classification_report(val_true_labels, val_predicted_labels, target_names=target_names)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjscynBYw7HA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5xR2FeE-XzAB",
        "w0dDo-x734m-"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
